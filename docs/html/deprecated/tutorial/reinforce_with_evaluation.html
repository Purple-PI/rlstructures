

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Evaluation of RL models in other processes &mdash; RLStructures  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> RLStructures
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview of rlstructures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gettingstarted/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../algorithms/index.html">Provided Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">RLStructures API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../foireaq/foireaq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Deprecated API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autoapi/index.html">API Reference</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">RLStructures</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Evaluation of RL models in other processes</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/deprecated/tutorial/reinforce_with_evaluation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="evaluation-of-rl-models-in-other-processes">
<h1>Evaluation of RL models in other processes<a class="headerlink" href="#evaluation-of-rl-models-in-other-processes" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/facebookresearch/rlstructures/tree/main/tutorial/tutorial_reinforce_with_evaluation">https://github.com/facebookresearch/rlstructures/tree/main/tutorial/tutorial_reinforce_with_evaluation</a></p></li>
</ul>
<p>Regarding the REINFORCE implementation, one missing aspect is a good evaluation of the policy:
* the evaluation has to be done with the <cite>deterministic</cite> policy (while learning is made with the stochastic policy)
* the evaluation over N episodes may be long, and we would like to avoid to slow down the learning</p>
<p>To solve this issue, we will use another batcher in <cite>asynchronous</cite> mode.</p>
<div class="section" id="creation-of-the-evaluation-batcher">
<h2>Creation of the evaluation batcher<a class="headerlink" href="#creation-of-the-evaluation-batcher" title="Permalink to this headline">¶</a></h2>
<p>The evaluation batcher can be created like the trainig batcher (but with a different number of threads and slots)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_model</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">=</span><span class="n">EpisodeBatcher</span><span class="p">(</span>
    <span class="n">n_timesteps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_episode_steps&quot;</span><span class="p">],</span>
    <span class="n">n_slots</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_episodes&quot;</span><span class="p">],</span>
    <span class="n">create_agent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_create_agent</span><span class="p">,</span>
    <span class="n">create_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">,</span>
    <span class="n">env_args</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;n_envs&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_envs&quot;</span><span class="p">],</span>
        <span class="s2">&quot;max_episode_steps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_episode_steps&quot;</span><span class="p">],</span>
        <span class="s2">&quot;env_name&quot;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;env_name&quot;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="n">agent_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;n_actions&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">},</span>
    <span class="n">n_threads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_threads&quot;</span><span class="p">],</span>
    <span class="n">seeds</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;env_seed&quot;</span><span class="p">]</span><span class="o">+</span><span class="n">k</span><span class="o">*</span><span class="mi">10</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_threads&quot;</span><span class="p">])],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="running-the-evaluation-batcher">
<h2>Running the evaluation batcher<a class="headerlink" href="#running-the-evaluation-batcher" title="Permalink to this headline">¶</a></h2>
<p>Running the evaluation batcher is made through <cite>execute</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_episodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_episodes&quot;</span><span class="p">]</span>
<span class="n">agent_info</span><span class="o">=</span><span class="n">DictTensor</span><span class="p">({</span><span class="s2">&quot;stochastic&quot;</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">)})</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">n_episodes</span><span class="o">=</span><span class="n">n_episodes</span><span class="p">,</span><span class="n">agent_info</span><span class="o">=</span><span class="n">agent_info</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluation_iteration</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>
</pre></div>
</div>
<p>Note that we store the iteration at which the evaluation batcher has been executed</p>
</div>
<div class="section" id="getting-trajectories-without-blocking-the-learning">
<h2>Getting trajectories without blocking the learning<a class="headerlink" href="#getting-trajectories-without-blocking-the-learning" title="Permalink to this headline">¶</a></h2>
<p>Not we can get episodes, but in non blocking mode: the batcher will return <cite>None</cite> if the process of computing episodes is not finished.
If the process is finished, we can 1) compute the reward 2) update the batchers models 3) relaunch the acquisition process. We thus have an evaluation process that runs without blocking the learning, and at maximum speed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">evaluation_trajectories</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">evaluation_trajectories</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="c1">#trajectories are available</span>
    <span class="c1">#Compute the cumulated reward</span>
    <span class="n">cumulated_reward</span><span class="o">=</span><span class="p">(</span><span class="n">evaluation_trajectories</span><span class="p">[</span><span class="s2">&quot;_reward&quot;</span><span class="p">]</span><span class="o">*</span><span class="n">evaluation_trajectories</span><span class="o">.</span><span class="n">mask</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;evaluation_reward&quot;</span><span class="p">,</span><span class="n">cumulated_reward</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_iteration</span><span class="p">)</span>
    <span class="c1">#We reexecute the evaluation batcher (with same value of agent_info and same number of episodes)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_iteration</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">reexecute</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2021, Facebook AI Research

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>